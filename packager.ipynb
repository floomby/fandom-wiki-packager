{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fandom (dot) Com Scraper + Package Builder\n",
    "\n",
    "This notebook can be used to scrape fandom (dot) com wiki pages in order to prepare a data package for the charlie discord bot.\n",
    "\n",
    "#### Usage\n",
    "\n",
    "This notebook does not automatically create an index of urls to scrape. In order to get started you must manually create the `urls.txt` file. This file should simply be a plaintext list of urls with one per line. Note that the order in which the urls are in the files matters. When running de-dupe the first file will be the one that keeps the data. [see here for more info](#de-dupe)\n",
    "\n",
    "A couple of the scripts in here make use of the OpenAI API. In order to use this they expect to find `OPENAI_API_KEY=\"<your key here>\"` in a file called `.env` in the working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep in case you do not have the nltk modules installed\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading\n",
    "\n",
    "Using pyppeteer to scrape has worked well for me. If working on other sites check the output of this step to see you are getting the data you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "import asyncio\n",
    "from pyppeteer import launch\n",
    "import os\n",
    "\n",
    "url_file = 'urls.txt'\n",
    "download_dir = 'downloaded_data'\n",
    "log_dir = 'logs'\n",
    "log_name = 'download_log'\n",
    "no_redownload = True\n",
    "\n",
    "# ensure the log directory exists\n",
    "if not path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "download_errors = open(path.join(log_dir, log_name), 'a', encoding='utf-8', newline='\\n')  # nopep8\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# ensure the download directory exists\n",
    "if not path.exists(download_dir):\n",
    "    os.mkdir(download_dir)\n",
    "\n",
    "\n",
    "async def download(url: str):\n",
    "    save_name = url.split('/')[-1].strip()\n",
    "    # colons are not allowed in filenames on windows...\n",
    "    save_name = save_name.replace(':', '__').replace('%27', '\\'') + '.html'\n",
    "\n",
    "    if no_redownload and path.exists(path.join(cwd, download_dir, save_name)):\n",
    "        # check if the file has content\n",
    "        with open(path.join(cwd, download_dir, save_name), 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        if content.__len__() != 0:\n",
    "            print(f'Skipping {url} because it already exists and is non-empty')\n",
    "            return\n",
    "\n",
    "    print(f'Downloading {url} to {path.join(cwd, download_dir, save_name)}')\n",
    "\n",
    "    browser = await launch()\n",
    "    page = await browser.newPage()\n",
    "    try:\n",
    "        await page.goto(url)\n",
    "        content = await page.content()\n",
    "\n",
    "        with open(path.join(download_dir, save_name), 'w', encoding='utf-8', newline='\\n') as f:\n",
    "            f.write(content)\n",
    "\n",
    "        if content.__len__() == 0:\n",
    "            print(f'Warning: {url} is empty')\n",
    "            download_errors.write(f'Warning: {url} is empty')\n",
    "        else:\n",
    "            print(f'Successfully downloaded {url}')\n",
    "    except Exception as e:\n",
    "        print(f'Error with {url}')\n",
    "        download_errors.write(f'Error with {url}')\n",
    "\n",
    "    finally:\n",
    "        await browser.close()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # read in the list of urls from a file\n",
    "    with open(url_file, 'r') as f:\n",
    "        urls = f.readlines()\n",
    "\n",
    "    # download each url\n",
    "    for i in range(urls.__len__()):\n",
    "        url = urls[i]\n",
    "        await download(url)\n",
    "        # wait a bit to be nice\n",
    "        if (i < urls.__len__() - 1):\n",
    "            await asyncio.sleep(5)\n",
    "\n",
    "await asyncio.get_event_loop().create_task(main())\n",
    "download_errors.close()\n",
    "\n",
    "print(f'Logged errors to {path.join(cwd, log_dir, log_name)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction\n",
    "\n",
    "We now want to extract the text content of the page disregarding the headers, footers and most of the navigation. This step is what makes this notebook highly specific fandom wiki pages. If you want to adapt this to other pages major changes will be needed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "url_file = 'urls.txt'\n",
    "download_dir = 'downloaded_data'\n",
    "log_dir = 'logs'\n",
    "extracted_data = 'extracted_data'\n",
    "log_name = 'extraction_log'\n",
    "\n",
    "# ensure the log directory exists\n",
    "if not path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "extraction_errors = open(path.join(log_dir, log_name), 'a', encoding='utf-8', newline='\\n')  # nopep8\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# ensure the extraction directory exists\n",
    "if not path.exists(extracted_data):\n",
    "    os.mkdir(extracted_data)\n",
    "\n",
    "\n",
    "# use the url file to get the list of files to extract\n",
    "with open(url_file, 'r') as f:\n",
    "    urls = f.readlines()\n",
    "\n",
    "for url in urls:\n",
    "    save_name = url.split('/')[-1].strip()\n",
    "    # colons are not allowed in filenames on windows...\n",
    "    save_name = save_name.replace(':', '__').replace('%27', '\\'')\n",
    "    input_file = save_name + '.html'\n",
    "    save_name = save_name + '.txt'\n",
    "\n",
    "    # read in the file\n",
    "    with open(path.join(download_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    soup = soup.find('div', {'class': 'mw-parser-output'})\n",
    "    # remove all tables with the class 'navbox'\n",
    "    for navbox in soup.find_all('table', {'class': 'navbox'}):\n",
    "        navbox.decompose()\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # remove lines with only whitespace\n",
    "    text = re.sub(r'^\\s+$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # if the line ends in \"[]\" we should insert a newline at the start of the line\n",
    "    text = re.sub(r'^(.*)(\\[\\])$', r'\\n\\1\\2', text, flags=re.MULTILINE)\n",
    "\n",
    "    # replace all 3+ newlines with a double newline\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "\n",
    "    if len(text) < 100:\n",
    "        print(f'Warning: low content in {save_name}')\n",
    "        extraction_errors.write(f'Warning: low content in {save_name}')\n",
    "\n",
    "    # save to file\n",
    "    with open(path.join(extracted_data, save_name), 'w', encoding='utf-8', newline='\\n') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    print(f'Extracted {path.join(cwd, extracted_data, save_name)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De-Dupe\n",
    "\n",
    "This is a step in which the ordering in the url list comes into play. The first file on the list will be the one that keeps the duplicated chunks. Intuitively the duplicated data is likely to be indexes/navigation, acknowledgements, branding stuff, and other low-detail content or even unwanted content. It makes sense for this data to be in the files which are indexes/navigation, overviews, or other miscellaneous pages. This will improve the high-detail information content density of the pages where we are most likely to  looking up specifics. This should result in better performance of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-duplication\n",
    "# chunks of text are separated by a blank line\n",
    "\n",
    "import os.path as path\n",
    "\n",
    "url_file = 'urls.txt'\n",
    "extracted_data = 'extracted_data'\n",
    "deduped_data = 'deduped_data'\n",
    "cwd = os.getcwd()\n",
    "verbose = True\n",
    "\n",
    "# ensure the deduplication directory exists\n",
    "if not path.exists(deduped_data):\n",
    "    os.mkdir(deduped_data)\n",
    "\n",
    "# de-duplicate the data\n",
    "\n",
    "hash_set = set()\n",
    "\n",
    "# use the url file to get the list of files to dedupe\n",
    "with open(url_file, 'r') as f:\n",
    "    urls = f.readlines()\n",
    "\n",
    "for url in urls:\n",
    "    save_name = url.split('/')[-1].strip()\n",
    "    # colons are not allowed in filenames on windows...\n",
    "    save_name = save_name.replace(':', '__').replace('%27', '\\'')\n",
    "    input_file = save_name + '.txt'\n",
    "    save_name = save_name + '.txt'\n",
    "\n",
    "    # read in the file\n",
    "    with open(path.join(extracted_data, input_file), 'r', encoding='utf-8', newline='\\n') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # split the content into chunks\n",
    "    chunks = content.split('\\n\\n')\n",
    "\n",
    "    # save the deduped data\n",
    "    save_file = open(path.join(deduped_data, save_name), 'w', encoding='utf-8', newline='\\n')  # nopep8\n",
    "\n",
    "    # add each chunk to the hash set\n",
    "    for chunk in chunks:\n",
    "        if chunk not in hash_set:\n",
    "            save_file.write(chunk + '\\n\\n')\n",
    "            hash_set.add(chunk)\n",
    "        elif verbose:\n",
    "            print(f'Duplicate chunk found: {chunk}')\n",
    "\n",
    "    print(f'Deduped {path.join(cwd, deduped_data, save_name)}')\n",
    "\n",
    "    save_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning\n",
    "\n",
    "Data cleaning can be done with a LLM.\n",
    "\n",
    "**Note:** This makes OpenAI calls and will need the API key set in the `.env` file.\n",
    "\n",
    "**WARNING!!!** This prompt is badly calibrated. Check that you are not getting garbage out of this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning prototyping\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import os.path as path\n",
    "\n",
    "url_file = 'urls.txt'\n",
    "deduped_data = 'deduped_data'\n",
    "cleaned_data = 'cleaned_data'\n",
    "log_dir = 'logs'\n",
    "log_name = 'cleaning_log'\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# ensure the log directory exists\n",
    "if not path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "# ensure the cleaning directory exists\n",
    "if not path.exists(cleaned_data):\n",
    "    os.mkdir(cleaned_data)\n",
    "\n",
    "# open the log file\n",
    "cleaning_errors = open(path.join(log_dir, log_name), 'a', encoding='utf-8', newline='\\n')  # nopep8\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "def clean(text, filename, line_number):\n",
    "    if text.__len__() == 0:\n",
    "        return text\n",
    "\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': f'Clean the following text, inserting spaces where it looks like spaces need to be inserted. Only output the cleaned text.\\n\\n{text}\\n\\n'},\n",
    "    ]\n",
    "\n",
    "    retry_count = 3\n",
    "\n",
    "    while retry_count > 0:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model='gpt-3.5-turbo-0613',\n",
    "                messages=messages,\n",
    "                temperature=0.0,\n",
    "                functions=[\n",
    "                    {\n",
    "                        \"name\": \"abort_cleaning\",\n",
    "                        \"description\": \"If the input text is already clean, abort the cleaning process.\",\n",
    "                        \"parameters\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {},\n",
    "                            \"required\": [],\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "                function_call=\"auto\",\n",
    "                # \"Clean\" - 28629\n",
    "                # \"clean\" - 18883\n",
    "                logit_bias={28629: -1, 18883: -1},\n",
    "            )\n",
    "\n",
    "            response_message = response[\"choices\"][0][\"message\"]  # type: ignore # nopep8\n",
    "            if response_message.get(\"function_call\"):\n",
    "                print(f'Aborting cleaning of {filename}:{line_number} >>> {text}')  # nopep8\n",
    "                return text\n",
    "            else:\n",
    "                return response_message[\"content\"].strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            if retry_count > 0:\n",
    "                print(f'Error: {e} (retrying)')\n",
    "                retry_count -= 1\n",
    "                continue\n",
    "            else:\n",
    "                print(f'Error: {e}')\n",
    "                cleaning_errors.write(f'Error in {filename}:{line_number} (cleaning line skipped): {e}\\n')  # nopep8\n",
    "    return text\n",
    "\n",
    "\n",
    "# use the url file to get the list of files to clean\n",
    "with open(url_file, 'r') as f:\n",
    "    urls = f.readlines()\n",
    "\n",
    "for url in urls:\n",
    "    save_name = url.split('/')[-1].strip()\n",
    "    # colons are not allowed in filenames on windows...\n",
    "    save_name = save_name.replace(':', '__').replace('%27', '\\'')\n",
    "    input_file = save_name + '.txt'\n",
    "    save_name = save_name + '.txt'\n",
    "\n",
    "    # read in the file\n",
    "    with open(path.join(deduped_data, input_file), 'r', encoding='utf-8', newline='\\n') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # we do the cleaning line by line for best results\n",
    "    lines = content.split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        lines[i] = clean(line, save_name, i)\n",
    "\n",
    "    # rejoin the lines\n",
    "    content = '\\n'.join(lines)\n",
    "\n",
    "    # save the cleaned data\n",
    "    with open(path.join(cleaned_data, save_name), 'w', encoding='utf-8', newline='\\n') as f:\n",
    "        f.write(content.strip())\n",
    "\n",
    "    print(f'Cleaned {path.join(cwd, deduped_data, save_name)} written to {path.join(cwd, cleaned_data, save_name)}')  # nopep8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition\n",
    "\n",
    "By running named entity recognition on the data we can create a decent way to preform activity detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME I have had problems with doing this step well, and so right now we are doing something very lazy\n",
    "\n",
    "import os\n",
    "\n",
    "url_file = 'urls.txt'\n",
    "named_entities = 'named_entities.txt'\n",
    "cwd = os.getcwd()\n",
    "\n",
    "with open(url_file, 'r') as f:\n",
    "    urls = f.readlines()\n",
    "\n",
    "output_file = open(named_entities, 'w', encoding='utf-8', newline='\\n')\n",
    "\n",
    "for url in urls:\n",
    "    entity = url.split('/')[-1].strip()\n",
    "    entity = entity.replace('%27', '\\'').replace('_', ' ')\n",
    "    output_file.write(entity + '\\n')\n",
    "\n",
    "output_file.close()\n",
    "\n",
    "print(f'Named entities written to {os.path.join(cwd, named_entities)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking\n",
    "\n",
    "We now need to break up the pages slightly overlapping into chunks for ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "import os\n",
    "import nltk\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "target_word_count_per_chunk = 300\n",
    "url_file = 'urls.txt'\n",
    "cleaned_data = 'cleaned_data'\n",
    "chunked_data = 'chunked_data'\n",
    "log_dir = 'logs'\n",
    "log_name = 'cleaning_log'\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# ensure the log directory exists\n",
    "if not path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "# ensure the chunking directory exists\n",
    "if not path.exists(chunked_data):\n",
    "    os.mkdir(chunked_data)\n",
    "\n",
    "# use the url file to get the list of files to clean\n",
    "with open(url_file, 'r') as f:\n",
    "    urls = f.readlines()\n",
    "\n",
    "for url in urls:\n",
    "    save_name = url.split('/')[-1].strip()\n",
    "    # colons are not allowed in filenames on windows...\n",
    "    save_name = save_name.replace(':', '__').replace('%27', '\\'')\n",
    "    input_file = save_name + '.txt'\n",
    "\n",
    "    page = open(path.join(cleaned_data, input_file), 'r', encoding='utf-8', newline='\\n')  # nopep8\n",
    "    text = page.read()\n",
    "    page.close()\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    current_chunk_word_count = 0\n",
    "    for sentence in sentences:\n",
    "        current_chunk += sentence\n",
    "        current_chunk_word_count += len(sentence.split(' '))\n",
    "        if current_chunk_word_count >= target_word_count_per_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk_word_count = 0\n",
    "            # overlap the chunks by 1 sentence\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            current_chunk += '\\n\\n'\n",
    "\n",
    "    # write the chunks to a file\n",
    "    i = 0\n",
    "    for chunk in chunks:\n",
    "        good_file_name = save_name.replace('__', ':')\n",
    "        good_file_name = save_name.replace('_', ' ')\n",
    "        outfile = open(path.join(chunked_data, f\"{save_name}_{i}.txt\"), 'w', encoding='utf-8', newline='\\n')  # nopep8\n",
    "        outfile.write(f\"{good_file_name} --------------\\n\\n\")\n",
    "        outfile.write(chunk)\n",
    "        outfile.close()\n",
    "        print(f'Chunked {path.join(cwd, cleaned_data, input_file)} into {path.join(cwd, chunked_data, save_name)}_{i}.txt')  # nopep8\n",
    "        i += 1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingesting and Exporting\n",
    "\n",
    "We now need to ingest the data into the vector store.\n",
    "\n",
    "I am going to redo the package format, but I have not done this yet. After I do this I will update this section with a cell that can do this. For right now however you need to use the `package_builder.js` script to create the package.\n",
    "\n",
    "```bash\n",
    "npm install\n",
    "npm run create -- my_package_name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a script to clean up all the temporary directories and files\n",
    "\n",
    "import os\n",
    "\n",
    "download_dir = 'downloaded_data'\n",
    "extracted_data = 'extracted_data'\n",
    "deduped_data = 'deduped_data'\n",
    "cleaned_data = 'cleaned_data'\n",
    "chunked_data = 'chunked_data'\n",
    "log_dir = 'logs'\n",
    "\n",
    "for dir in [download_dir, extracted_data, deduped_data, cleaned_data, chunked_data, log_dir]:\n",
    "    if os.path.exists(dir):\n",
    "        for file in os.listdir(dir):\n",
    "            os.remove(os.path.join(dir, file))\n",
    "        os.rmdir(dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
